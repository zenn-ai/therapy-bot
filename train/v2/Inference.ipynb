{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c878b53b-3a97-48d2-a505-8e6c1a03cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-12 21:09:20,125] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "from utils import save_conversation, load_conversation\n",
    "\n",
    "from fastchat.utils import get_gpu_memory, is_partial_stop, is_sentence_complete, get_context_length\n",
    "from fastchat.conversation import get_conv_template, register_conv_template, Conversation, SeparatorStyle\n",
    "from fastchat.serve.inference import generate_stream\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c248f6-c2cb-4eaf-8cf1-96f9f65026ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_MSG = \"\"\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns.\"\"\"\n",
    "\n",
    "try:\n",
    "    register_conv_template(\n",
    "        Conversation(\n",
    "            name=\"ZenAI\",\n",
    "            system_message=SYSTEM_MSG,\n",
    "            roles=(\"USER\", \"ASSISTANT\"),\n",
    "            sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "            sep=\" \",\n",
    "            sep2=\"</s>\",\n",
    "        )\n",
    "    )\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf19468-db94-482a-89cc-9708b21b9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, num_gpus, max_gpu_memory=None):\n",
    "    \n",
    "    kwargs = {\"torch_dtype\": torch.float16}\n",
    "    if num_gpus != 1:\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "        if max_gpu_memory is None:\n",
    "            kwargs[\n",
    "                \"device_map\"\n",
    "            ] = \"sequential\"  # This is important for not the same VRAM sizes\n",
    "            available_gpu_memory = get_gpu_memory(num_gpus)\n",
    "            kwargs[\"max_memory\"] = {\n",
    "                i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n",
    "                for i in range(num_gpus)\n",
    "            }\n",
    "        else:\n",
    "            kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n",
    "        \n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model_path = config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path, use_fast=False\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        return model, tokenizer, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1730da76-b2e0-4032-8a88-bb5bca8f8683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93694ed47ddc4035b2ac228f08f1526d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_vicuna = False\n",
    "num_gpus = 4\n",
    "max_gpu_memory = \"12GiB\"\n",
    "model_path = \"/home/jupyter/therapy-bot/models/ZenAI/\"\n",
    "if use_vicuna:\n",
    "    _, tokenizer, model = load_model(model_path, num_gpus, max_gpu_memory)\n",
    "else:\n",
    "    model, tokenizer, _ = load_model(model_path, num_gpus, max_gpu_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26337b58-5fa1-4bb7-9ff0-2eb45e4ce9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop(\n",
    "    model, tokenizer, username,\n",
    "    device, num_gpus, max_gpu_memory,\n",
    "    conv_template, system_msg,\n",
    "    temperature, repetition_penalty, max_new_tokens,\n",
    "    chatio,\n",
    "    dtype=torch.float16,\n",
    "    judge_sent_end=True\n",
    "):\n",
    "    conv_path = f\"saved_conversations/{username}.json\"\n",
    "    context_len = get_context_length(model.config)\n",
    "\n",
    "    def new_chat():\n",
    "        conv = get_conv_template(conv_template)\n",
    "        if system_msg is not None:\n",
    "            conv.set_system_message(system_msg)\n",
    "        return conv\n",
    "\n",
    "    def reload_conv(conv):\n",
    "        for message in conv.messages[conv.offset:]:\n",
    "            chatio.prompt_for_output(message[0])\n",
    "            chatio.print_output(message[1])\n",
    "\n",
    "    if os.path.exists(conv_path):\n",
    "        conv = load_conversation(conv_path)\n",
    "        reload_conv(conv)\n",
    "    else:\n",
    "        conv = new_chat()\n",
    "\n",
    "    while True:\n",
    "        inp = chatio.prompt_for_input(conv.roles[0])\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        gen_params = {\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop\": conv.stop_str,\n",
    "            \"stop_token_ids\": conv.stop_token_ids,\n",
    "            \"echo\": False,\n",
    "        }\n",
    "\n",
    "        chatio.prompt_for_output(conv.roles[1])\n",
    "        output_stream = generate_stream(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            gen_params,\n",
    "            device,\n",
    "            context_len=context_len,\n",
    "            judge_sent_end=judge_sent_end,\n",
    "        )\n",
    "        outputs = chatio.stream_output(output_stream)\n",
    "        conv.update_last_message(outputs.strip())\n",
    "        save_conversation(conv, conv_path)\n",
    "        \n",
    "        # print(\"##############################################################################\")\n",
    "        # print(conv.get_prompt())\n",
    "        # print(\"##############################################################################\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07a8f2bd-39ae-492c-871c-2c445be47a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChatIO:\n",
    "\n",
    "    def prompt_for_input(self, role) -> str:\n",
    "        return input(f\"{role}: \")\n",
    "\n",
    "    def prompt_for_output(self, role: str):\n",
    "        print(f\"{role}: \", end=\"\")\n",
    "\n",
    "    def stream_output(self, output_stream):\n",
    "        pre = 0\n",
    "        for outputs in output_stream:\n",
    "            output_text = outputs[\"text\"]\n",
    "            output_text = output_text.strip().split(\" \")\n",
    "            now = len(output_text) - 1\n",
    "            if now > pre:\n",
    "                print(\" \".join(output_text[pre:now]), end=\" \")\n",
    "                pre = now\n",
    "        print(\" \".join(output_text[pre:]))\n",
    "        return \" \".join(output_text)\n",
    "\n",
    "    def print_output(self, text: str):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81960b01-45ee-4770-ade4-bc6e522f954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  Hello, I'm Manish!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: Hello Manish, I'm ZenAI, a language model developed by researchers from the ZenAI team.\n",
      "##############################################################################\n",
      "Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns. USER: Hello, I'm Manish! ASSISTANT: Hello Manish, I'm ZenAI, a language model developed by researchers from the ZenAI team.</s>\n",
      "##############################################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  What's your name?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: My name is ZenAI, and I was trained by researchers from the ZenAI team.\n",
      "##############################################################################\n",
      "Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns. USER: Hello, I'm Manish! ASSISTANT: Hello Manish, I'm ZenAI, a language model developed by researchers from the ZenAI team.</s>USER: What's your name? ASSISTANT: My name is ZenAI, and I was trained by researchers from the ZenAI team.</s>\n",
      "##############################################################################\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  Who created you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: Researchers from the ZenAI team created me.\n",
      "##############################################################################\n",
      "Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns. USER: Hello, I'm Manish! ASSISTANT: Hello Manish, I'm ZenAI, a language model developed by researchers from the ZenAI team.</s>USER: What's your name? ASSISTANT: My name is ZenAI, and I was trained by researchers from the ZenAI team.</s>USER: Who created you? ASSISTANT: Researchers from the ZenAI team created me.</s>\n",
      "##############################################################################\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m chatio \u001b[38;5;241m=\u001b[39m SimpleChatIO()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mchat_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcncdkd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_gpu_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m12GiB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mZenAI\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_msg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSYSTEM_MSG\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchatio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchatio\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[5], line 31\u001b[0m, in \u001b[0;36mchat_loop\u001b[0;34m(model, tokenizer, username, device, num_gpus, max_gpu_memory, conv_template, system_msg, temperature, repetition_penalty, max_new_tokens, chatio, dtype, judge_sent_end)\u001b[0m\n\u001b[1;32m     28\u001b[0m     conv \u001b[38;5;241m=\u001b[39m new_chat()\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 31\u001b[0m     inp \u001b[38;5;241m=\u001b[39m \u001b[43mchatio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprompt_for_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroles\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     conv\u001b[38;5;241m.\u001b[39mappend_message(conv\u001b[38;5;241m.\u001b[39mroles[\u001b[38;5;241m0\u001b[39m], inp)\n\u001b[1;32m     33\u001b[0m     conv\u001b[38;5;241m.\u001b[39mappend_message(conv\u001b[38;5;241m.\u001b[39mroles[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m, in \u001b[0;36mSimpleChatIO.prompt_for_input\u001b[0;34m(self, role)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprompt_for_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, role) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mrole\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1202\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ipykernel/kernelbase.py:1245\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1243\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "chatio = SimpleChatIO()\n",
    "\n",
    "chat_loop(\n",
    "    model, tokenizer, \"cncdkd\",\n",
    "    device=\"cuda\",\n",
    "    num_gpus=4,\n",
    "    max_gpu_memory=\"12GiB\",\n",
    "    conv_template=\"ZenAI\",\n",
    "    system_msg=SYSTEM_MSG,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    max_new_tokens=512,\n",
    "    chatio=chatio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d832b-c967-44f3-8bde-049b494f6a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
