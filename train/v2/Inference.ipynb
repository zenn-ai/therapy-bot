{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c878b53b-3a97-48d2-a505-8e6c1a03cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-30 04:38:54,055] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from peft import PeftConfig, PeftModel\n",
    "\n",
    "from utils import save_conversation, load_conversation\n",
    "\n",
    "from fastchat.utils import get_gpu_memory, is_partial_stop, is_sentence_complete, get_context_length\n",
    "from fastchat.conversation import get_conv_template, register_conv_template, Conversation, SeparatorStyle\n",
    "from fastchat.serve.inference import generate_stream\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c248f6-c2cb-4eaf-8cf1-96f9f65026ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    register_conv_template(\n",
    "        Conversation(\n",
    "            name=\"ZenAI\",\n",
    "            system_message=\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns.\",\n",
    "            roles=(\"USER\", \"ASSISTANT\"),\n",
    "            sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "            sep=\" \",\n",
    "            sep2=\"</s>\",\n",
    "        )\n",
    "    )\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bf19468-db94-482a-89cc-9708b21b9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, num_gpus, max_gpu_memory=None):\n",
    "    \n",
    "    kwargs = {\"torch_dtype\": torch.float16}\n",
    "    if num_gpus != 1:\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "        if max_gpu_memory is None:\n",
    "            kwargs[\n",
    "                \"device_map\"\n",
    "            ] = \"sequential\"  # This is important for not the same VRAM sizes\n",
    "            available_gpu_memory = get_gpu_memory(num_gpus)\n",
    "            kwargs[\"max_memory\"] = {\n",
    "                i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n",
    "                for i in range(num_gpus)\n",
    "            }\n",
    "        else:\n",
    "            kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n",
    "        \n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model_path = config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path, use_fast=False\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        return model, tokenizer, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1730da76-b2e0-4032-8a88-bb5bca8f8683",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e647b66466d485492bc81e935a44824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "use_vicuna = False\n",
    "num_gpus = 4\n",
    "max_gpu_memory = \"12GiB\"\n",
    "model_path = \"/home/jupyter/therapy-bot/models/zenai_sample/\"\n",
    "if use_vicuna:\n",
    "    _, tokenizer, model = load_model(model_path, num_gpus, max_gpu_memory)\n",
    "else:\n",
    "    model, tokenizer, _ = load_model(model_path, num_gpus, max_gpu_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "26337b58-5fa1-4bb7-9ff0-2eb45e4ce9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop(\n",
    "    model, tokenizer, username,\n",
    "    device, num_gpus, max_gpu_memory,\n",
    "    conv_template, system_msg,\n",
    "    temperature, repetition_penalty, max_new_tokens,\n",
    "    chatio,\n",
    "    dtype=torch.float16,\n",
    "    judge_sent_end=True\n",
    "):\n",
    "    conv_path = f\"saved_conversations/{username}.json\"\n",
    "    context_len = get_context_length(model.config)\n",
    "\n",
    "    def new_chat():\n",
    "        conv = get_conv_template(conv_template)\n",
    "        if system_msg is not None:\n",
    "            conv.set_system_message(system_msg)\n",
    "        return conv\n",
    "\n",
    "    def reload_conv(conv):\n",
    "        for message in conv.messages[conv.offset:]:\n",
    "            chatio.prompt_for_output(message[0])\n",
    "            chatio.print_output(message[1])\n",
    "\n",
    "    if os.path.exists(conv_path):\n",
    "        conv = load_conversation(conv_path)\n",
    "        reload_conv(conv)\n",
    "    else:\n",
    "        conv = new_chat()\n",
    "\n",
    "    while True:\n",
    "\n",
    "        inp = chatio.prompt_for_input(conv.roles[0])\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        gen_params = {\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop\": conv.stop_str,\n",
    "            \"stop_token_ids\": conv.stop_token_ids,\n",
    "            \"echo\": False,\n",
    "        }\n",
    "\n",
    "        chatio.prompt_for_output(conv.roles[1])\n",
    "        output_stream = generate_stream(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            gen_params,\n",
    "            device,\n",
    "            context_len=context_len,\n",
    "            judge_sent_end=judge_sent_end,\n",
    "        )\n",
    "        outputs = chatio.stream_output(output_stream)\n",
    "        conv.update_last_message(outputs.strip())\n",
    "        save_conversation(conv, conv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07a8f2bd-39ae-492c-871c-2c445be47a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChatIO:\n",
    "\n",
    "    def prompt_for_input(self, role) -> str:\n",
    "        return input(f\"{role}: \")\n",
    "\n",
    "    def prompt_for_output(self, role: str):\n",
    "        print(f\"{role}: \", end=\"\")\n",
    "\n",
    "    def stream_output(self, output_stream):\n",
    "        pre = 0\n",
    "        for outputs in output_stream:\n",
    "            output_text = outputs[\"text\"]\n",
    "            output_text = output_text.strip().split(\" \")\n",
    "            now = len(output_text) - 1\n",
    "            if now > pre:\n",
    "                print(\" \".join(output_text[pre:now]), end=\" \")\n",
    "                pre = now\n",
    "        print(\" \".join(output_text[pre:]))\n",
    "        return \" \".join(output_text)\n",
    "\n",
    "    def print_output(self, text: str):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81960b01-45ee-4770-ade4-bc6e522f954e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER: Hello!\n",
      "ASSISTANT: Hello! How can I help you today?\n",
      "USER: I'm Manish. Who are you?\n",
      "ASSISTANT: My name is ZenAI, and I'm a therapist developed by researchers from the ZenAI team..\n",
      "USER: Hello!\n",
      "ASSISTANT: Hello! How can I help you today?\n",
      "USER: Who am I?\n",
      "ASSISTANT: You are Manish, a new patient who recently joined the ZenAI team.\n",
      "USER: Hola\n",
      "ASSISTANT: Hola! ¿En qué puedo ayudarte hoy?\n",
      "USER: I don't know Spanish\n",
      "ASSISTANT: That's okay, I can communicate with you in English. How can I assist you today?\n",
      "USER: helo\n",
      "ASSISTANT: Hello! Is there anything I can help you with?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  Hello\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: Hello! Is there anything on your mind that you would like to discuss?\n"
     ]
    }
   ],
   "source": [
    "chatio = SimpleChatIO()\n",
    "\n",
    "chat_loop(\n",
    "    model, tokenizer, \"manish\",\n",
    "    device=\"cuda\",\n",
    "    num_gpus=4,\n",
    "    max_gpu_memory=\"12GiB\",\n",
    "    conv_template=\"ZenAI\",\n",
    "    system_msg=\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns.\",\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    max_new_tokens=512,\n",
    "    chatio=chatio\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8d832b-c967-44f3-8bde-049b494f6a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
