{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c878b53b-3a97-48d2-a505-8e6c1a03cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-16 14:05:55,106] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Inference for FastChat models.\"\"\"\n",
    "import abc\n",
    "import gc\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from typing import Iterable, Optional, Dict\n",
    "import warnings\n",
    "\n",
    "import psutil\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    ")\n",
    "from transformers.generation.logits_process import (\n",
    "    LogitsProcessorList,\n",
    "    RepetitionPenaltyLogitsProcessor,\n",
    "    TemperatureLogitsWarper,\n",
    "    TopKLogitsWarper,\n",
    "    TopPLogitsWarper,\n",
    ")\n",
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf19468-db94-482a-89cc-9708b21b9a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_memory(max_gpus=None):\n",
    "    \"\"\"Get available memory for each GPU.\"\"\"\n",
    "    import torch\n",
    "\n",
    "    gpu_memory = []\n",
    "    num_gpus = (\n",
    "        torch.cuda.device_count()\n",
    "        if max_gpus is None\n",
    "        else min(max_gpus, torch.cuda.device_count())\n",
    "    )\n",
    "\n",
    "    for gpu_id in range(num_gpus):\n",
    "        with torch.cuda.device(gpu_id):\n",
    "            device = torch.cuda.current_device()\n",
    "            gpu_properties = torch.cuda.get_device_properties(device)\n",
    "            total_memory = gpu_properties.total_memory / (1024**3)\n",
    "            allocated_memory = torch.cuda.memory_allocated() / (1024**3)\n",
    "            available_memory = total_memory - allocated_memory\n",
    "            gpu_memory.append(available_memory)\n",
    "    return gpu_memory\n",
    "\n",
    "def load_model(model_path, num_gpus, max_gpu_memory=None):\n",
    "    \n",
    "    kwargs = {\"torch_dtype\": torch.float16}\n",
    "    if num_gpus != 1:\n",
    "        kwargs[\"device_map\"] = \"auto\"\n",
    "        if max_gpu_memory is None:\n",
    "            kwargs[\n",
    "                \"device_map\"\n",
    "            ] = \"sequential\"  # This is important for not the same VRAM sizes\n",
    "            available_gpu_memory = get_gpu_memory(num_gpus)\n",
    "            kwargs[\"max_memory\"] = {\n",
    "                i: str(int(available_gpu_memory[i] * 0.85)) + \"GiB\"\n",
    "                for i in range(num_gpus)\n",
    "            }\n",
    "        else:\n",
    "            kwargs[\"max_memory\"] = {i: max_gpu_memory for i in range(num_gpus)}\n",
    "        \n",
    "        config = PeftConfig.from_pretrained(model_path)\n",
    "        base_model_path = config.base_model_name_or_path\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            base_model_path, use_fast=False\n",
    "        )\n",
    "        base_model = AutoModelForCausalLM.from_pretrained(\n",
    "            base_model_path,\n",
    "            low_cpu_mem_usage=True,\n",
    "            **kwargs,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(base_model, model_path)\n",
    "        \n",
    "        return model, tokenizer, base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e319fa7c-cab6-423a-a90d-5b2aed20fc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_partial_stop(output: str, stop_str: str):\n",
    "    \"\"\"Check whether the output contains a partial stop str.\"\"\"\n",
    "    for i in range(0, min(len(output), len(stop_str))):\n",
    "        if stop_str.startswith(output[-i:]):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def is_sentence_complete(output: str):\n",
    "    \"\"\"Check whether the output is a complete sentence.\"\"\"\n",
    "    end_symbols = (\".\", \"?\", \"!\", \"...\", \"。\", \"？\", \"！\", \"…\", '\"', \"'\", \"”\")\n",
    "    return output.endswith(end_symbols)\n",
    "\n",
    "SEQUENCE_LENGTH_KEYS = [\n",
    "    \"max_sequence_length\",\n",
    "    \"seq_length\",\n",
    "    \"max_position_embeddings\",\n",
    "    \"max_seq_len\",\n",
    "    \"model_max_length\",\n",
    "]\n",
    "\n",
    "\n",
    "def get_context_length(config):\n",
    "    \"\"\"Get the context length of a model from a huggingface model config.\"\"\"\n",
    "    rope_scaling = getattr(config, \"rope_scaling\", None)\n",
    "    if rope_scaling:\n",
    "        rope_scaling_factor = config.rope_scaling[\"factor\"]\n",
    "    else:\n",
    "        rope_scaling_factor = 1\n",
    "\n",
    "    for key in SEQUENCE_LENGTH_KEYS:\n",
    "        val = getattr(config, key, None)\n",
    "        if val is not None:\n",
    "            return int(rope_scaling_factor * val)\n",
    "    return 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "877347ee-3c0b-462a-9e03-513df1f18ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_logits_processor(\n",
    "    temperature: float, repetition_penalty: float, top_p: float, top_k: int\n",
    ") -> LogitsProcessorList:\n",
    "    processor_list = LogitsProcessorList()\n",
    "    # TemperatureLogitsWarper doesn't accept 0.0, 1.0 makes it a no-op so we skip two cases.\n",
    "    if temperature >= 1e-5 and temperature != 1.0:\n",
    "        processor_list.append(TemperatureLogitsWarper(temperature))\n",
    "    if repetition_penalty > 1.0:\n",
    "        processor_list.append(RepetitionPenaltyLogitsProcessor(repetition_penalty))\n",
    "    if 1e-8 <= top_p < 1.0:\n",
    "        processor_list.append(TopPLogitsWarper(top_p))\n",
    "    if top_k > 0:\n",
    "        processor_list.append(TopKLogitsWarper(top_k))\n",
    "    return processor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9ece84e-eed3-4c46-9aec-e60aaa54b8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.inference_mode()\n",
    "def generate_stream(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    params: Dict,\n",
    "    device: str,\n",
    "    context_len: int,\n",
    "    stream_interval: int = 2,\n",
    "    judge_sent_end: bool = False,\n",
    "):\n",
    "    if hasattr(model, \"device\"):\n",
    "        device = model.device\n",
    "\n",
    "    # Read parameters\n",
    "    prompt = params[\"prompt\"]\n",
    "    len_prompt = len(prompt)\n",
    "    temperature = float(params.get(\"temperature\", 1.0))\n",
    "    repetition_penalty = float(params.get(\"repetition_penalty\", 1.0))\n",
    "    top_p = float(params.get(\"top_p\", 1.0))\n",
    "    top_k = int(params.get(\"top_k\", -1))  # -1 means disable\n",
    "    max_new_tokens = int(params.get(\"max_new_tokens\", 256))\n",
    "    echo = bool(params.get(\"echo\", True))\n",
    "    stop_str = params.get(\"stop\", None)\n",
    "    stop_token_ids = params.get(\"stop_token_ids\", None) or []\n",
    "    if tokenizer.eos_token_id not in stop_token_ids:\n",
    "        stop_token_ids.append(tokenizer.eos_token_id)\n",
    "\n",
    "    logits_processor = prepare_logits_processor(\n",
    "        temperature, repetition_penalty, top_p, top_k\n",
    "    )\n",
    "    input_ids = tokenizer(prompt).input_ids\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        max_src_len = context_len\n",
    "    else:  # truncate\n",
    "        max_src_len = context_len - max_new_tokens - 1\n",
    "\n",
    "    input_ids = input_ids[-max_src_len:]\n",
    "    output_ids = list(input_ids)\n",
    "    input_echo_len = len(input_ids)\n",
    "\n",
    "    if model.config.is_encoder_decoder:\n",
    "        encoder_output = model.encoder(\n",
    "            input_ids=torch.as_tensor([input_ids], device=device)\n",
    "        )[0]\n",
    "        start_ids = torch.as_tensor(\n",
    "            [[model.generation_config.decoder_start_token_id]],\n",
    "            dtype=torch.int64,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "    past_key_values = out = None\n",
    "    sent_interrupt = False\n",
    "    finish_reason = None\n",
    "    for i in range(max_new_tokens):\n",
    "        if i == 0:  # prefill\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=start_ids,\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                )\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(torch.as_tensor([input_ids], device=device), use_cache=True)\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "        else:  # decoding\n",
    "            if model.config.is_encoder_decoder:\n",
    "                out = model.decoder(\n",
    "                    input_ids=torch.as_tensor(\n",
    "                        [[token] if not sent_interrupt else output_ids],\n",
    "                        device=device,\n",
    "                    ),\n",
    "                    encoder_hidden_states=encoder_output,\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "\n",
    "                logits = model.lm_head(out[0])\n",
    "            else:\n",
    "                out = model(\n",
    "                    input_ids=torch.as_tensor(\n",
    "                        [[token] if not sent_interrupt else output_ids],\n",
    "                        device=device,\n",
    "                    ),\n",
    "                    use_cache=True,\n",
    "                    past_key_values=past_key_values if not sent_interrupt else None,\n",
    "                )\n",
    "                sent_interrupt = False\n",
    "                logits = out.logits\n",
    "            past_key_values = out.past_key_values\n",
    "\n",
    "        if logits_processor:\n",
    "            if repetition_penalty > 1.0:\n",
    "                tmp_output_ids = torch.as_tensor([output_ids], device=logits.device)\n",
    "            else:\n",
    "                tmp_output_ids = None\n",
    "            last_token_logits = logits_processor(tmp_output_ids, logits[:, -1, :])[0]\n",
    "        else:\n",
    "            last_token_logits = logits[0, -1, :]\n",
    "\n",
    "        if temperature < 1e-5 or top_p < 1e-8:  # greedy\n",
    "            _, indices = torch.topk(last_token_logits, 2)\n",
    "            tokens = [int(index) for index in indices.tolist()]\n",
    "        else:\n",
    "            probs = torch.softmax(last_token_logits, dim=-1)\n",
    "            indices = torch.multinomial(probs, num_samples=2)\n",
    "            tokens = [int(token) for token in indices.tolist()]\n",
    "        token = tokens[0]\n",
    "        output_ids.append(token)\n",
    "\n",
    "        if token in stop_token_ids:\n",
    "            stopped = True\n",
    "        else:\n",
    "            stopped = False\n",
    "\n",
    "        # Yield the output tokens\n",
    "        if i % stream_interval == 0 or i == max_new_tokens - 1 or stopped:\n",
    "            if echo:\n",
    "                tmp_output_ids = output_ids\n",
    "                rfind_start = len_prompt\n",
    "            else:\n",
    "                tmp_output_ids = output_ids[input_echo_len:]\n",
    "                rfind_start = 0\n",
    "\n",
    "            output = tokenizer.decode(\n",
    "                tmp_output_ids,\n",
    "                skip_special_tokens=True,\n",
    "                spaces_between_special_tokens=False,\n",
    "                clean_up_tokenization_spaces=True,\n",
    "            )\n",
    "            # TODO: For the issue of incomplete sentences interrupting output, apply a patch and others can also modify it to a more elegant way\n",
    "            if judge_sent_end and stopped and not is_sentence_complete(output):\n",
    "                if len(tokens) > 1:\n",
    "                    token = tokens[1]\n",
    "                    output_ids[-1] = token\n",
    "                else:\n",
    "                    output_ids.pop()\n",
    "                stopped = False\n",
    "                sent_interrupt = True\n",
    "\n",
    "            partially_stopped = False\n",
    "            if stop_str:\n",
    "                if isinstance(stop_str, str):\n",
    "                    pos = output.rfind(stop_str, rfind_start)\n",
    "                    if pos != -1:\n",
    "                        output = output[:pos]\n",
    "                        stopped = True\n",
    "                    else:\n",
    "                        partially_stopped = is_partial_stop(output, stop_str)\n",
    "                elif isinstance(stop_str, Iterable):\n",
    "                    for each_stop in stop_str:\n",
    "                        pos = output.rfind(each_stop, rfind_start)\n",
    "                        if pos != -1:\n",
    "                            output = output[:pos]\n",
    "                            stopped = True\n",
    "                            break\n",
    "                        else:\n",
    "                            partially_stopped = is_partial_stop(output, each_stop)\n",
    "                            if partially_stopped:\n",
    "                                break\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid stop field type.\")\n",
    "\n",
    "            # Prevent yielding partial stop sequence\n",
    "            if not partially_stopped:\n",
    "                yield {\n",
    "                    \"text\": output,\n",
    "                    \"usage\": {\n",
    "                        \"prompt_tokens\": input_echo_len,\n",
    "                        \"completion_tokens\": i,\n",
    "                        \"total_tokens\": input_echo_len + i,\n",
    "                    },\n",
    "                    \"finish_reason\": None,\n",
    "                }\n",
    "\n",
    "        if stopped:\n",
    "            break\n",
    "\n",
    "    # Finish stream event, which contains finish reason\n",
    "    else:\n",
    "        finish_reason = \"length\"\n",
    "\n",
    "    if stopped:\n",
    "        finish_reason = \"stop\"\n",
    "\n",
    "    yield {\n",
    "        \"text\": output,\n",
    "        \"usage\": {\n",
    "            \"prompt_tokens\": input_echo_len,\n",
    "            \"completion_tokens\": i,\n",
    "            \"total_tokens\": input_echo_len + i,\n",
    "        },\n",
    "        \"finish_reason\": finish_reason,\n",
    "    }\n",
    "\n",
    "    # Clean\n",
    "    del past_key_values, out\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e6741ec-6f92-488c-96a6-08f044120446",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatIO(abc.ABC):\n",
    "    @abc.abstractmethod\n",
    "    def prompt_for_input(self, role: str) -> str:\n",
    "        \"\"\"Prompt for input from a role.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def prompt_for_output(self, role: str):\n",
    "        \"\"\"Prompt for output from a role.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def stream_output(self, output_stream):\n",
    "        \"\"\"Stream output.\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def print_output(self, text: str):\n",
    "        \"\"\"Print output.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26337b58-5fa1-4bb7-9ff0-2eb45e4ce9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop(\n",
    "    model_path: str,\n",
    "    device: str,\n",
    "    num_gpus: int,\n",
    "    max_gpu_memory: str,\n",
    "    dtype: Optional[torch.dtype],\n",
    "    conv_template: Optional[str],\n",
    "    conv_system_msg: Optional[str],\n",
    "    temperature: float,\n",
    "    repetition_penalty: float,\n",
    "    max_new_tokens: int,\n",
    "    chatio: ChatIO,\n",
    "    judge_sent_end: bool = True,\n",
    "    history: bool = True,\n",
    "    base_model=False\n",
    "):\n",
    "    # Model\n",
    "    if base_model:\n",
    "        _, tokenizer, model = load_model(model_path, num_gpus, max_gpu_memory)\n",
    "    else:\n",
    "        model, tokenizer, _ = load_model(model_path, num_gpus, max_gpu_memory)\n",
    "\n",
    "    model_type = str(type(model)).lower()\n",
    "\n",
    "    # Set context length\n",
    "    context_len = get_context_length(model.config)\n",
    "\n",
    "    # Chat\n",
    "    def new_chat():\n",
    "        conv = get_conv_template(conv_template)\n",
    "        if conv_system_msg is not None:\n",
    "            conv.set_system_message(conv_system_msg)\n",
    "        return conv\n",
    "\n",
    "    def reload_conv(conv):\n",
    "        \"\"\"\n",
    "        Reprints the conversation from the start.\n",
    "        \"\"\"\n",
    "        for message in conv.messages[conv.offset :]:\n",
    "            chatio.prompt_for_output(message[0])\n",
    "            chatio.print_output(message[1])\n",
    "\n",
    "    conv = None\n",
    "\n",
    "    while True:\n",
    "        if not history or not conv:\n",
    "            conv = new_chat()\n",
    "\n",
    "        try:\n",
    "            inp = chatio.prompt_for_input(conv.roles[0])\n",
    "        except EOFError:\n",
    "            inp = \"\"\n",
    "\n",
    "        if inp == \"!!exit\" or not inp:\n",
    "            print(\"exit...\")\n",
    "            break\n",
    "        elif inp == \"!!reset\":\n",
    "            print(\"resetting...\")\n",
    "            conv = new_chat()\n",
    "            continue\n",
    "        elif inp.startswith(\"!!save\"):\n",
    "            args = inp.split(\" \", 1)\n",
    "\n",
    "            if len(args) != 2:\n",
    "                print(\"usage: !!save <filename>\")\n",
    "                continue\n",
    "            else:\n",
    "                filename = args[1]\n",
    "\n",
    "            # Add .json if extension not present\n",
    "            if not \".\" in filename:\n",
    "                filename += \".json\"\n",
    "\n",
    "            print(\"saving...\", filename)\n",
    "            with open(filename, \"w\") as outfile:\n",
    "                json.dump(conv.dict(), outfile)\n",
    "            continue\n",
    "        elif inp.startswith(\"!!load\"):\n",
    "            args = inp.split(\" \", 1)\n",
    "\n",
    "            if len(args) != 2:\n",
    "                print(\"usage: !!load <filename>\")\n",
    "                continue\n",
    "            else:\n",
    "                filename = args[1]\n",
    "\n",
    "            # Check if file exists and add .json if needed\n",
    "            if not os.path.exists(filename):\n",
    "                if (not filename.endswith(\".json\")) and os.path.exists(\n",
    "                    filename + \".json\"\n",
    "                ):\n",
    "                    filename += \".json\"\n",
    "                else:\n",
    "                    print(\"file not found:\", filename)\n",
    "                    continue\n",
    "\n",
    "            print(\"loading...\", filename)\n",
    "            with open(filename, \"r\") as infile:\n",
    "                new_conv = json.load(infile)\n",
    "\n",
    "            conv = get_conv_template(new_conv[\"template_name\"])\n",
    "            conv.set_system_message(new_conv[\"system_message\"])\n",
    "            conv.messages = new_conv[\"messages\"]\n",
    "            reload_conv(conv)\n",
    "            continue\n",
    "\n",
    "        conv.append_message(conv.roles[0], inp)\n",
    "        conv.append_message(conv.roles[1], None)\n",
    "        prompt = conv.get_prompt()\n",
    "\n",
    "        gen_params = {\n",
    "            \"model\": model_path,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"repetition_penalty\": repetition_penalty,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "            \"stop\": conv.stop_str,\n",
    "            \"stop_token_ids\": conv.stop_token_ids,\n",
    "            \"echo\": False,\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            chatio.prompt_for_output(conv.roles[1])\n",
    "            output_stream = generate_stream(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                gen_params,\n",
    "                device,\n",
    "                context_len=context_len,\n",
    "                judge_sent_end=judge_sent_end,\n",
    "            )\n",
    "            t = time.time()\n",
    "            outputs = chatio.stream_output(output_stream)\n",
    "            duration = time.time() - t\n",
    "            conv.update_last_message(outputs.strip())\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"stopped generation.\")\n",
    "            # If generation didn't finish\n",
    "            if conv.messages[-1][1] is None:\n",
    "                conv.messages.pop()\n",
    "                # Remove last user message, so there isn't a double up\n",
    "                if conv.messages[-1][0] == conv.roles[0]:\n",
    "                    conv.messages.pop()\n",
    "\n",
    "                reload_conv(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07a8f2bd-39ae-492c-871c-2c445be47a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleChatIO(ChatIO):\n",
    "\n",
    "    def prompt_for_input(self, role) -> str:\n",
    "        return input(f\"{role}: \")\n",
    "\n",
    "    def prompt_for_output(self, role: str):\n",
    "        print(f\"{role}: \", end=\"\", flush=True)\n",
    "\n",
    "    def stream_output(self, output_stream):\n",
    "        pre = 0\n",
    "        for outputs in output_stream:\n",
    "            output_text = outputs[\"text\"]\n",
    "            output_text = output_text.strip().split(\" \")\n",
    "            now = len(output_text) - 1\n",
    "            if now > pre:\n",
    "                print(\" \".join(output_text[pre:now]), end=\" \", flush=True)\n",
    "                pre = now\n",
    "        print(\" \".join(output_text[pre:]), flush=True)\n",
    "        return \" \".join(output_text)\n",
    "\n",
    "    def print_output(self, text: str):\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4be2969c-f3f4-4aeb-85cb-43f1acdbc918",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatio = SimpleChatIO()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07604f6a-e707-419f-a2b2-845fd9621a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Conversation prompt templates.\n",
    "\n",
    "We kindly request that you import fastchat instead of copying this file if you wish to use it.\n",
    "If you have any changes in mind, please contribute back so the community can benefit collectively and continue to maintain these valuable templates.\n",
    "\"\"\"\n",
    "\n",
    "import dataclasses\n",
    "from enum import auto, IntEnum\n",
    "from typing import List, Any, Dict, Union, Tuple\n",
    "\n",
    "\n",
    "class SeparatorStyle(IntEnum):\n",
    "    \"\"\"Separator styles.\"\"\"\n",
    "\n",
    "    ADD_COLON_SINGLE = auto()\n",
    "    ADD_COLON_TWO = auto()\n",
    "    ADD_COLON_SPACE_SINGLE = auto()\n",
    "    NO_COLON_SINGLE = auto()\n",
    "    NO_COLON_TWO = auto()\n",
    "    ADD_NEW_LINE_SINGLE = auto()\n",
    "    LLAMA2 = auto()\n",
    "    CHATGLM = auto()\n",
    "    CHATML = auto()\n",
    "    CHATINTERN = auto()\n",
    "    DOLLY = auto()\n",
    "    RWKV = auto()\n",
    "    PHOENIX = auto()\n",
    "    ROBIN = auto()\n",
    "    FALCON_CHAT = auto()\n",
    "\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class Conversation:\n",
    "    \"\"\"A class that manages prompt templates and keeps all conversation history.\"\"\"\n",
    "\n",
    "    # The name of this template\n",
    "    name: str\n",
    "    # The template of the system prompt\n",
    "    system_template: str = \"{system_message}\"\n",
    "    # The system message\n",
    "    system_message: str = \"\"\n",
    "    # The names of two roles\n",
    "    roles: Tuple[str] = (\"USER\", \"ASSISTANT\")\n",
    "    # All messages. Each item is (role, message).\n",
    "    messages: List[List[str]] = ()\n",
    "    # The number of few shot examples\n",
    "    offset: int = 0\n",
    "    # The separator style and configurations\n",
    "    sep_style: SeparatorStyle = SeparatorStyle.ADD_COLON_SINGLE\n",
    "    sep: str = \"\\n\"\n",
    "    sep2: str = None\n",
    "    # Stop criteria (the default one is EOS token)\n",
    "    stop_str: Union[str, List[str]] = None\n",
    "    # Stops generation if meeting any token in this list\n",
    "    stop_token_ids: List[int] = None\n",
    "\n",
    "    def get_prompt(self) -> str:\n",
    "        \"\"\"Get the prompt for generation.\"\"\"\n",
    "        system_prompt = self.system_template.format(system_message=self.system_message)\n",
    "        if self.sep_style == SeparatorStyle.ADD_COLON_SINGLE:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_COLON_TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt + seps[0]\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_COLON_SPACE_SINGLE:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \": \"  # must be end with a space\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ADD_NEW_LINE_SINGLE:\n",
    "            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \"\\n\" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \"\\n\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.NO_COLON_SINGLE:\n",
    "            ret = system_prompt\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + message + self.sep\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.NO_COLON_TWO:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += role\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.RWKV:\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += (\n",
    "                        role\n",
    "                        + \": \"\n",
    "                        + message.replace(\"\\r\\n\", \"\\n\").replace(\"\\n\\n\", \"\\n\")\n",
    "                    )\n",
    "                    ret += \"\\n\\n\"\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.LLAMA2:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            if self.system_message:\n",
    "                ret = system_prompt\n",
    "            else:\n",
    "                ret = \"[INST] \"\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                tag = self.roles[i % 2]\n",
    "                if message:\n",
    "                    if i == 0:\n",
    "                        ret += message + \" \"\n",
    "                    else:\n",
    "                        ret += tag + \" \" + message + seps[i % 2]\n",
    "                else:\n",
    "                    ret += tag\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATGLM:\n",
    "            # source: https://huggingface.co/THUDM/chatglm-6b/blob/1d240ba371910e9282298d4592532d7f0f3e9f3e/modeling_chatglm.py#L1302-L1308\n",
    "            # source2: https://huggingface.co/THUDM/chatglm2-6b/blob/e186c891cf64310ac66ef10a87e6635fa6c2a579/modeling_chatglm.py#L926\n",
    "            round_add_n = 1 if self.name == \"chatglm2\" else 0\n",
    "            if system_prompt:\n",
    "                ret = system_prompt + self.sep\n",
    "            else:\n",
    "                ret = \"\"\n",
    "\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if i % 2 == 0:\n",
    "                    ret += f\"[Round {i//2 + round_add_n}]{self.sep}\"\n",
    "\n",
    "                if message:\n",
    "                    ret += f\"{role}：{message}{self.sep}\"\n",
    "                else:\n",
    "                    ret += f\"{role}：\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATML:\n",
    "            ret = \"\" if system_prompt == \"\" else system_prompt + self.sep + \"\\n\"\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \"\\n\" + message + self.sep + \"\\n\"\n",
    "                else:\n",
    "                    ret += role + \"\\n\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.CHATINTERN:\n",
    "            # source: https://huggingface.co/internlm/internlm-chat-7b-8k/blob/bd546fa984b4b0b86958f56bf37f94aa75ab8831/modeling_internlm.py#L771\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if i % 2 == 0:\n",
    "                    ret += \"<s>\"\n",
    "                if message:\n",
    "                    ret += role + \":\" + message + seps[i % 2] + \"\\n\"\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.DOLLY:\n",
    "            seps = [self.sep, self.sep2]\n",
    "            ret = system_prompt\n",
    "            for i, (role, message) in enumerate(self.messages):\n",
    "                if message:\n",
    "                    ret += role + \":\\n\" + message + seps[i % 2]\n",
    "                    if i % 2 == 1:\n",
    "                        ret += \"\\n\\n\"\n",
    "                else:\n",
    "                    ret += role + \":\\n\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.PHOENIX:\n",
    "            ret = system_prompt\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \": \" + \"<s>\" + message + \"</s>\"\n",
    "                else:\n",
    "                    ret += role + \": \" + \"<s>\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.ROBIN:\n",
    "            ret = system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \":\\n\" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\\n\"\n",
    "            return ret\n",
    "        elif self.sep_style == SeparatorStyle.FALCON_CHAT:\n",
    "            ret = \"\"\n",
    "            if self.system_message:\n",
    "                ret += system_prompt + self.sep\n",
    "            for role, message in self.messages:\n",
    "                if message:\n",
    "                    ret += role + \": \" + message + self.sep\n",
    "                else:\n",
    "                    ret += role + \":\"\n",
    "\n",
    "            return ret\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid style: {self.sep_style}\")\n",
    "\n",
    "    def set_system_message(self, system_message: str):\n",
    "        \"\"\"Set the system message.\"\"\"\n",
    "        self.system_message = system_message\n",
    "\n",
    "    def append_message(self, role: str, message: str):\n",
    "        \"\"\"Append a new message.\"\"\"\n",
    "        self.messages.append([role, message])\n",
    "\n",
    "    def update_last_message(self, message: str):\n",
    "        \"\"\"Update the last output.\n",
    "\n",
    "        The last message is typically set to be None when constructing the prompt,\n",
    "        so we need to update it in-place after getting the response from a model.\n",
    "        \"\"\"\n",
    "        self.messages[-1][1] = message\n",
    "\n",
    "    def to_gradio_chatbot(self):\n",
    "        \"\"\"Convert the conversation to gradio chatbot format.\"\"\"\n",
    "        ret = []\n",
    "        for i, (role, msg) in enumerate(self.messages[self.offset :]):\n",
    "            if i % 2 == 0:\n",
    "                ret.append([msg, None])\n",
    "            else:\n",
    "                ret[-1][-1] = msg\n",
    "        return ret\n",
    "\n",
    "    def to_openai_api_messages(self):\n",
    "        \"\"\"Convert the conversation to OpenAI chat completion format.\"\"\"\n",
    "        ret = [{\"role\": \"system\", \"content\": self.system_message}]\n",
    "\n",
    "        for i, (_, msg) in enumerate(self.messages[self.offset :]):\n",
    "            if i % 2 == 0:\n",
    "                ret.append({\"role\": \"user\", \"content\": msg})\n",
    "            else:\n",
    "                if msg is not None:\n",
    "                    ret.append({\"role\": \"assistant\", \"content\": msg})\n",
    "        return ret\n",
    "\n",
    "    def copy(self):\n",
    "        return Conversation(\n",
    "            name=self.name,\n",
    "            system_template=self.system_template,\n",
    "            system_message=self.system_message,\n",
    "            roles=self.roles,\n",
    "            messages=[[x, y] for x, y in self.messages],\n",
    "            offset=self.offset,\n",
    "            sep_style=self.sep_style,\n",
    "            sep=self.sep,\n",
    "            sep2=self.sep2,\n",
    "            stop_str=self.stop_str,\n",
    "            stop_token_ids=self.stop_token_ids,\n",
    "        )\n",
    "\n",
    "    def dict(self):\n",
    "        return {\n",
    "            \"template_name\": self.name,\n",
    "            \"system_message\": self.system_message,\n",
    "            \"roles\": self.roles,\n",
    "            \"messages\": self.messages,\n",
    "            \"offset\": self.offset,\n",
    "        }\n",
    "\n",
    "\n",
    "# A global registry for all conversation templates\n",
    "conv_templates: Dict[str, Conversation] = {}\n",
    "\n",
    "\n",
    "def register_conv_template(template: Conversation, override: bool = False):\n",
    "    \"\"\"Register a new conversation template.\"\"\"\n",
    "    if not override:\n",
    "        assert (\n",
    "            template.name not in conv_templates\n",
    "        ), f\"{template.name} has been registered.\"\n",
    "\n",
    "    conv_templates[template.name] = template\n",
    "\n",
    "\n",
    "def get_conv_template(name: str) -> Conversation:\n",
    "    \"\"\"Get a conversation template.\"\"\"\n",
    "    return conv_templates[name].copy()\n",
    "\n",
    "register_conv_template(\n",
    "    Conversation(\n",
    "        name=\"vicuna_v1.1\",\n",
    "        system_message=\"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "        \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "        roles=(\"USER\", \"ASSISTANT\"),\n",
    "        sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "        sep=\" \",\n",
    "        sep2=\"</s>\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "562b25bc-f3ff-4877-af16-2bdf081689af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat_loop(\n",
    "#     model_path=\"/home/jupyter/therapy-bot/models/zenai_sample/\",\n",
    "#     device=\"cuda\",\n",
    "#     num_gpus=4,\n",
    "#     max_gpu_memory=\"12GiB\",\n",
    "#     dtype=torch.float16,\n",
    "#     conv_template=\"vicuna_v1.1\",\n",
    "#     conv_system_msg=\"A chat between a curious user and an artificial intelligence assistant. \"\n",
    "#         \"The assistant gives helpful, detailed, and polite answers to the user's questions.\",\n",
    "#     temperature=0.7,\n",
    "#     repetition_penalty=1.0,\n",
    "#     max_new_tokens=512,\n",
    "#     chatio=chatio,\n",
    "#     judge_sent_end=True,\n",
    "#     history=True,\n",
    "#     base_model=True\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81960b01-45ee-4770-ade4-bc6e522f954e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee6d1c979304f39a294d15550c8e55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  Hello!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: Hello! How can I help you today? Let me know if you have any questions or need help with anything. I'm here to assist.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "USER:  Who are you?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASSISTANT: You can call me Vicuna, and I was trained by Large Model Systems Organization (LMSYS) researchers as a language model.\n"
     ]
    }
   ],
   "source": [
    "chat_loop(\n",
    "    model_path=\"/home/jupyter/therapy-bot/models/zenai_sample/\",\n",
    "    device=\"cuda\",\n",
    "    num_gpus=4,\n",
    "    max_gpu_memory=\"12GiB\",\n",
    "    dtype=torch.float16,\n",
    "    conv_template=\"vicuna_v1.1\",\n",
    "    conv_system_msg=\"Your name is ZenAI and you're a licensed therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns.\",\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.0,\n",
    "    max_new_tokens=512,\n",
    "    chatio=chatio,\n",
    "    judge_sent_end=True,\n",
    "    history=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
