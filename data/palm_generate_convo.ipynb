{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "373e4330-e4ef-4edc-807d-e8ef30945f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install google-cloud-aiplatform==1.25.0\n",
    "# %pip install google-api-core==1.33.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21f6f6d7-5555-46ca-bfc6-fc43b8fd7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import vertexai\n",
    "from vertexai.preview.language_models import TextGenerationModel\n",
    "import pandas as pd\n",
    "from PyPDF2 import PdfReader\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ed35db3-4ea4-4028-95f9-e0ae018d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_parse(pdf_file):\n",
    "    with open(pdf_file, 'rb') as pdf:\n",
    "        reader = PdfReader(pdf, strict=False)\n",
    "        pdf_text = []\n",
    "        for page in reader.pages:\n",
    "            content = page.extract_text()\n",
    "            pdf_text.append(content)\n",
    "        return \" \".join(pdf_text)\n",
    "    \n",
    "def predict_large_language_model_sample(\n",
    "    project_id: str,\n",
    "    model_name: str,\n",
    "    temperature: float,\n",
    "    max_output_tokens: int,\n",
    "    top_p: float,\n",
    "    top_k: int,\n",
    "    content: str,\n",
    "    location: str = \"us-central1\",\n",
    "    tuned_model_name: str = \"\",\n",
    "    ) :\n",
    "    \n",
    "    vertexai.init(project=project_id, location=location)\n",
    "    model = TextGenerationModel.from_pretrained(model_name)\n",
    "    if tuned_model_name:\n",
    "        model = model.get_tuned_model(tuned_model_name)\n",
    "    response = model.predict(\n",
    "        content,\n",
    "        temperature=temperature,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1067a8be-c927-4ead-b356-52c8fee500d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convo_df(text):\n",
    "    exchanges = [exchange.strip() for exchange in text.split('\\n\\n') if exchange.strip()]\n",
    "    zen_responses = []\n",
    "    client_responses = []\n",
    "    for exchange in exchanges:\n",
    "        if exchange.startswith(\"**Zen:**\"):\n",
    "            zen_responses.append(exchange.replace('**Zen:**', '').strip())\n",
    "        elif exchange.startswith(\"**Client:**\"):\n",
    "            client_responses.append(exchange.replace('**Client:**', '').strip())\n",
    "\n",
    "    max_length = max(len(zen_responses), len(client_responses))\n",
    "    zen_responses.extend([''] * (max_length - len(zen_responses)))\n",
    "    client_responses.extend([''] * (max_length - len(client_responses)))\n",
    "\n",
    "    df = pd.DataFrame({'Client': client_responses,'Zen': zen_responses, })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "024b395e-19e8-487c-a7ae-e6032881e780",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'Alexander Street/Batch 4/'\n",
    "all_files = os.listdir(path)\n",
    "pdf_files = [filename for filename in all_files if filename.lower().endswith('.pdf')]\n",
    "source = [os.path.splitext(pdf_file)[0] for pdf_file in pdf_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "735cefa5-eb22-4581-99a4-d40e4e61f610",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "            ### Prompt: Using the sample therapy transcript below, generate a synthetic therapy transcript\\n\n",
    "            between a mobile based AI therapist Zen and a client. Understand the client's struggles from the sample\\n\n",
    "            and then make Zen utilize state-of-the-art therapeutic techniques such as motivational interviewing.\\n\n",
    "            Zen should be empathetic and a great listener.\\n\n",
    "            The flow of the transcript needs to be mobile friendly and engaging.\\n\n",
    "            \n",
    "            Use the format \"**Client:**\" and \"**Zen:**\"\\n\n",
    "            ----------------------------------------------------------------------------\n",
    "            ### Transcript:\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "1c032085-2625-473d-b23d-f4ecf8b01084",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(len(source)):\n",
    "    src = source[i]\n",
    "    conv_id = i+1\n",
    "    transcript = pdf_parse(path+src+'.pdf')\n",
    "    text = predict_large_language_model_sample(project_id = \"cloud-lab-ff59\", \n",
    "                                    model_name = \"text-bison\", \n",
    "                                    temperature = 0.4, \n",
    "                                    max_output_tokens = 1024,\n",
    "                                    top_p = 0.8, \n",
    "                                    top_k = 40, \n",
    "                                    location = \"us-central1\",\n",
    "                                    content = prompt + transcript)\n",
    "    temp = convo_df(text)\n",
    "    temp['conv_id'] = conv_id\n",
    "    temp['source'] = src\n",
    "    temp = temp[['conv_id','Client', 'Zen', 'source']]\n",
    "    \n",
    "    df = df.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3a37a205-b2e1-458a-a6f3-62e582ab84ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"df-batch-4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "916c3620-f092-4ebb-8ae3-b66aaa51e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"df-batch-1.csv\")\n",
    "df_2 = pd.read_csv(\"df-batch-2.csv\")\n",
    "df_3 = pd.read_csv(\"df-batch-3.csv\")\n",
    "df_3 = pd.read_csv(\"df-batch-4.csv\")\n",
    "df = pd.concat([df_1, df_2, df_3])\n",
    "source_to_number = {}\n",
    "unique_sources = df['source'].unique()\n",
    "for i, source in enumerate(unique_sources):\n",
    "    source_to_number[source] = i + 1\n",
    "df['conv_id'] = df['source'].map(source_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "c6f480fb-8fe8-4f31-bed9-367817215047",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cc4dd0b2-b0b0-4aaa-95eb-14f3575728bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(483, 4)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "0d4ee023-d8d6-4028-b921-f6b1ccbed64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['source'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e2271-9832-4e8a-80a2-c8781bb4e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"Client\": \"USER\", \"Zen\": \"ASSISTANT\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c3418a21-8d18-440b-aecb-b1645a288e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"processed/PALM_Alexander_Street.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9064e625-1e7f-44a1-8627-0966b814e161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e3823ab-812f-4c0a-9576-ec8b138434a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dd484ef-a400-4d40-adf5-8575117f2fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.conversation import get_conv_template, register_conv_template, Conversation, SeparatorStyle\n",
    "SYSTEM_MSG = \"\"\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd7bfad5-defa-40bc-b449-3fa9114839c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    register_conv_template(\n",
    "        Conversation(\n",
    "            name=\"ZenAI\",\n",
    "            system_message=SYSTEM_MSG,\n",
    "            roles=(\"USER\", \"ASSISTANT\"),\n",
    "            sep_style=SeparatorStyle.ADD_COLON_TWO,\n",
    "            sep=\" \",\n",
    "            sep2=\"</s>\",\n",
    "        )\n",
    "    )\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "16b7dbae-32af-4f89-9a81-9548f320e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df():\n",
    "    csv_files = [\"mental_health_chatbot_dataset.csv\", \"psychology-dataset.csv\", \"who_r_u.csv\"]\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for p in csv_files:\n",
    "        df1 = pd.read_csv(f\"../data/processed/{p}\")[[\"human\", \"zen\"]]\n",
    "        df1 = df1.rename(columns={\"human\": \"USER\", \"zen\": \"ASSISTANT\"})\n",
    "        df1 = df1.drop_duplicates(subset=[\"USER\", \"ASSISTANT\"], keep=\"first\", ignore_index=True)\n",
    "\n",
    "        df1[\"USER\"] = df1.USER.str.replace('\\s+', ' ', regex=True)\n",
    "        df1[\"USER\"] = df1.USER.str.replace(r'\\.([a-zA-Z0-9])', r'. \\1', regex=True)\n",
    "        df1[\"ASSISTANT\"] = df1.ASSISTANT.str.replace('\\s+', ' ', regex=True)\n",
    "        df1[\"ASSISTANT\"] = df1.ASSISTANT.str.replace(r'\\.([a-zA-Z0-9])', r'. \\1', regex=True)\n",
    "\n",
    "        df = pd.concat([df, df1])\n",
    "\n",
    "    df = df.sample(frac=1, random_state=42)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_conversations(df, reset):\n",
    "    conv = get_conv_template(\"ZenAI\")\n",
    "    conversations = []\n",
    "    \n",
    "    conv.messages = []\n",
    "    for index, row in df.iterrows():\n",
    "        if reset:\n",
    "            conv.messages = []\n",
    "        conv.append_message(\"USER\", row[\"USER\"])\n",
    "        conv.append_message(\"ASSISTANT\", row[\"ASSISTANT\"])\n",
    "        conversations.append(conv.get_prompt())\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "\n",
    "def get_therapy_conv():\n",
    "    df = pd.read_csv(\"../data/processed/PALM_Alexander_Street.csv\")\n",
    "    df = df.dropna(ignore_index=True)\n",
    "    df = df.drop(index=0)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    conv_ids = df.conv_id.unique()\n",
    "    conversations = []\n",
    "    for c in conv_ids:\n",
    "        conv_df = df[df.conv_id == c]\n",
    "        conversations += get_conversations(conv_df, reset=False)\n",
    "    \n",
    "    return conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cfd3d819-c061-4667-9f52-1eec63654e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2fa2b79f-15d0-48e8-b5e9-024823a09c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df()\n",
    "conversations = get_conversations(df, reset=True)\n",
    "conversations += get_therapy_conv()\n",
    "\n",
    "train, test = train_test_split(conversations, test_size=1000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "44d92bcc-023b-4e19-b141-50fddcd07faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10905, 9905, 1000)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(conversations), len(train), len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0810c2cf-82ef-4b02-a1e3-1e0b25d35c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-11-05 02:29:34,908] [INFO] [real_accelerator.py:158:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    PreTrainedTokenizer\n",
    ")\n",
    "from transformers.trainer_pt_utils import LabelSmoother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fa7c82f8-443e-428d-b058-b29ef8958bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IGNORE_TOKEN_ID = LabelSmoother.ignore_index\n",
    "model_id = \"lmsys/vicuna-13b-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e38339fe-a131-4f51-9255-50a5cef8c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    model_max_length=1024,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bd4de86-73ac-4343-a086-75e8c3ffee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def rank0_print(*args):\n",
    "    print(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6d3b85cd-8c13-4b1f-b04e-99a03b74b595",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversations = conversations[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4617f48d-bdae-4577-820c-80f1e3e25a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize conversations\n",
    "input_ids = tokenizer(\n",
    "    conversations,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    ").input_ids\n",
    "targets = input_ids.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a30db6a5-0c35-434a-8a9b-ce117891f558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   1, 3575, 1024,  ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "64a7ce60-fcef-4220-9764-18199391b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "sources = [example[\"conversations\"] for example in raw_data][0:1]\n",
    "\n",
    "conv = get_conversation_template(\"vicuna\")\n",
    "roles = {\"human\": conv.roles[0], \"gpt\": conv.roles[1]}\n",
    "\n",
    "# Apply prompt templates\n",
    "conversations = []\n",
    "for i, source in enumerate(sources):\n",
    "    if roles[source[0][\"from\"]] != conv.roles[0]:\n",
    "        # Skip the first one if it is not from human\n",
    "        source = source[1:]\n",
    "\n",
    "    conv.messages = []\n",
    "    for j, sentence in enumerate(source):\n",
    "        role = roles[sentence[\"from\"]]\n",
    "        assert role == conv.roles[j % 2], f\"{i}\"\n",
    "        conv.append_message(role, sentence[\"value\"])\n",
    "    conversations.append(conv.get_prompt())\n",
    "\n",
    "# Tokenize conversations\n",
    "input_ids = tokenizer(\n",
    "    conversations,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=\"max_length\",\n",
    "    max_length=tokenizer.model_max_length,\n",
    "    truncation=True,\n",
    ").input_ids\n",
    "targets = input_ids.clone()\n",
    "\n",
    "assert conv.sep_style == SeparatorStyle.ADD_COLON_TWO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c77c9b90-3f07-4e08-94ee-d6f4fe662b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True,  ..., True, True, True])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0] == input_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b259b842-9315-44fa-96b6-aa26b651dc39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' ASSISTANT: '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv.sep + conv.roles[1] + \": \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0c2e9e21-0242-424e-8150-4a4001dcb019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask targets. Only compute loss on the assistant outputs.\n",
    "sep = conv.sep + conv.roles[1] + \": \"\n",
    "for conversation, target in zip(conversations, targets):\n",
    "    total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "    turns = conversation.split(conv.sep2)\n",
    "    cur_len = 1\n",
    "    target[:cur_len] = IGNORE_TOKEN_ID\n",
    "    for i, turn in enumerate(turns):\n",
    "        if turn == \"\":\n",
    "            break\n",
    "        turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "        parts = turn.split(sep)\n",
    "        if len(parts) != 2:\n",
    "            break\n",
    "        parts[0] += sep\n",
    "        # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n",
    "        instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "        if i != 0 and not tokenizer.legacy:\n",
    "            # The legacy and non-legacy modes handle special tokens differently\n",
    "            instruction_len -= 1\n",
    "\n",
    "        # Ignore the user instructions\n",
    "        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "        cur_len += turn_len\n",
    "\n",
    "        if i != 0 and not tokenizer.legacy:\n",
    "            # The legacy and non-legacy modes handle special tokens differently\n",
    "            cur_len -= 1\n",
    "\n",
    "    target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "    if False:  # Inspect and check the correctness of masking\n",
    "        z = target.clone()\n",
    "        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "        rank0_print(tokenizer.decode(z))\n",
    "        exit()\n",
    "\n",
    "    if cur_len < tokenizer.model_max_length:\n",
    "        if cur_len != total_len:\n",
    "            target[:] = IGNORE_TOKEN_ID\n",
    "            rank0_print(\n",
    "                f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "                f\" #turn = {len(turns) - 1}. (ignored)\"\n",
    "            )\n",
    "\n",
    "data = dict(\n",
    "    input_ids=input_ids,\n",
    "    labels=targets,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8e938ad5-75aa-4e4a-b9eb-00cafceee5d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It can be difficult to come out to loved ones, but it's important to prioritize your own happiness and well-being. Let's work together to explore some techniques to improve your confidence and prepare for coming out. Have you considered seeking support groups or practicing self-care?\""
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "01c1a2d6-0793-47e3-b3cf-c787dfae478a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk> It can be difficult to come out to loved ones, but it's important to prioritize your own happiness and well-being. Let's work together to explore some techniques to improve your confidence and prepare for coming out. Have you considered seeking support groups or practicing self-care?</s><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n"
     ]
    }
   ],
   "source": [
    "z = target.clone()\n",
    "z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "rank0_print(tokenizer.decode(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "0d925c36-f619-45ac-a722-9bbac853625f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(124, 124)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_len, cur_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "181aa715-564e-40ce-a2a0-bfefbf1d8bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6946cb91-2fdb-430d-a9e0-96b6e8d6db75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(62)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(target != -100).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4351672b-d06b-4b5b-aa23-5ea820f56ee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns. USER: I am struggling with my sexuality and don't know how to come out to my family. ASSIATANT: It can be difficult to come out to loved ones, but it's important to prioritize your own happiness and well-being. Let's work together to explore some techniques to improve your confidence and prepare for coming out. Have you considered seeking support groups or practicing self-care?\"]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn.split(sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "e0a437bb-03f9-420b-96b7-d40ae71b952e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(targets != -100).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b71f5e3c-c463-4139-bc69-c23a88febf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).\",\n",
       " 'USER: Have a nice day! ASSISTANT: You too!',\n",
       " '']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turns = conversations[0].split(conv.sep2)\n",
    "turns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "248b9133-46bc-4015-be66-94dffc88bfcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "turn = turns[0]\n",
    "len(tokenizer(turn).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "21cb344b-8aa9-40fc-aa1d-7c4904f07c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Who are you?\",\n",
       " 'I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parts = turn.split(sep)\n",
    "parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6a77cb47-24d3-4250-a4eb-078edeaf761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "parts[0] += sep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f0758187-b2a2-404d-a190-73938a444d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer(parts[0]).input_ids) - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fc2e2bac-e5ca-4cdc-b5d4-3eb96bccb162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "00d7df10-031b-4d64-93d9-2b63a437a660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1024]), torch.Size([1, 1024]), torch.Size([1, 1024]))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels'].shape, data['input_ids'].shape, data['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c56f475d-7330-4a1a-b44c-081d2a2ab7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).</s>USER: Have a nice day! ASSISTANT: You too!</s>\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8a96be23-d6ba-4506-9481-1a2b9bd7ee88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(84)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['attention_mask'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "283132cc-0ea1-435e-8eee-8b176e78826c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 3575, 1024,  ...,    0,    0,    0]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f6da27e-637b-4a78-9a42-4ac8e6928289",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastchat.model.model_adapter import get_conversation_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27e9bdcb-3f93-4567-8819-facd803d52b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conversation(name='vicuna_v1.1', system_template='{system_message}', system_message=\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\", roles=('USER', 'ASSISTANT'), messages=[], offset=0, sep_style=<SeparatorStyle.ADD_COLON_TWO: 2>, sep=' ', sep2='</s>', stop_str=None, stop_token_ids=None)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_conversation_template('vicuna')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff56a6a5-31cf-4e6e-b1d6-df37c6b17d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "raw_data = json.load(open(\"../train/v2/dummy_conversation.json\", \"r\"))\n",
    "sources = [example[\"conversations\"] for example in raw_data]\n",
    "c, t = preprocess(sources, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5138b19c-2e1a-4f96-ad4e-17031968824b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c), len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2475b335-7537-49fc-92ef-57b1424b885b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: Who are you? ASSISTANT: I am Vicuna, a language model trained by researchers from Large Model Systems Organization (LMSYS).</s>USER: Have a nice day! ASSISTANT: You too!</s>\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "636b8d76-5da1-4b3f-b0fe-55aa7fd58cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Your name is ZenAI and you're a therapist. Please have a conversation with your patient and provide them with a helpful response to their concerns. USER: I am struggling with my sexuality and don't know how to come out to my family. ASSIATANT: It can be difficult to come out to loved ones, but it's important to prioritize your own happiness and well-being. Let's work together to explore some techniques to improve your confidence and prepare for coming out. Have you considered seeking support groups or practicing self-care?</s>\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b37075-637f-48d8-bc41-84d014dbb269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "908db562-a6cb-435b-81d4-b79cedb071cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37e5381d-935b-4e6c-bdd7-af59692fd0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mask targets. Only compute loss on the assistant outputs.\n",
    "sep = \" ASSISTANT: \"\n",
    "for conversation, target in zip(conversations, targets):\n",
    "    total_len = int(target.ne(tokenizer.pad_token_id).sum())\n",
    "\n",
    "    turns = conversation.split(\"</s>\")\n",
    "    cur_len = 1\n",
    "    target[:cur_len] = IGNORE_TOKEN_ID\n",
    "    for i, turn in enumerate(turns):\n",
    "        if turn == \"\":\n",
    "            break\n",
    "        turn_len = len(tokenizer(turn).input_ids)\n",
    "\n",
    "        parts = turn.split(sep)\n",
    "        if len(parts) != 2:\n",
    "            break\n",
    "        parts[0] += sep\n",
    "        # \"-2\" is hardcoded for the Llama tokenizer to make the offset correct.\n",
    "        instruction_len = len(tokenizer(parts[0]).input_ids) - 2\n",
    "\n",
    "        if i != 0 and not tokenizer.legacy:\n",
    "            # The legacy and non-legacy modes handle special tokens differently\n",
    "            instruction_len -= 1\n",
    "\n",
    "        # Ignore the user instructions\n",
    "        target[cur_len : cur_len + instruction_len] = IGNORE_TOKEN_ID\n",
    "        cur_len += turn_len\n",
    "\n",
    "        if i != 0 and not tokenizer.legacy:\n",
    "            # The legacy and non-legacy modes handle special tokens differently\n",
    "            cur_len -= 1\n",
    "\n",
    "    target[cur_len:] = IGNORE_TOKEN_ID\n",
    "\n",
    "    if True:  # Inspect and check the correctness of masking\n",
    "        z = target.clone()\n",
    "        z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)\n",
    "        rank0_print(tokenizer.decode(z))\n",
    "        exit()\n",
    "\n",
    "#     if cur_len < tokenizer.model_max_length:\n",
    "#         if cur_len != total_len:\n",
    "#             target[:] = IGNORE_TOKEN_ID\n",
    "#             rank0_print(\n",
    "#                 f\"WARNING: tokenization mismatch: {cur_len} vs. {total_len}.\"\n",
    "#                 f\" #turn = {len(turns) - 1}. (ignored)\"\n",
    "#             )\n",
    "\n",
    "data = dict(\n",
    "    input_ids=input_ids,\n",
    "    labels=targets,\n",
    "    attention_mask=input_ids.ne(tokenizer.pad_token_id),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe2f8ca-ea5f-4fdd-b006-d47df4078764",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1, 3575, 1024,  ...,    0,    0,    0]]),\n",
       " 'labels': tensor([[-100, -100, -100,  ..., -100, -100, -100]]),\n",
       " 'attention_mask': tensor([[ True,  True,  True,  ..., False, False, False]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "768b00aa-5ca4-406a-b4b2-87fe20b4a8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = target.clone()\n",
    "z = torch.where(z == IGNORE_TOKEN_ID, tokenizer.unk_token_id, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5d18ce0-9bf4-4bd9-93e4-37b9531caced",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2251eb65-9632-47fd-9ca7-3cb9a1ae133c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_len"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-gpu.m112",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-gpu:m112"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
